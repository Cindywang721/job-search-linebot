import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime
import re


class JobCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def delay_random(self):
        """éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«åçˆ¬èŸ²åµæ¸¬"""
        time.sleep(random.uniform(1, 3))

    def clean_text(self, text):
        """æ¸…ç†æ–‡å­—ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())

    def extract_salary(self, salary_text):
        """æå–è–ªè³‡è³‡è¨Š"""
        if not salary_text:
            return "é¢è­°"

        # ç§»é™¤å¸¸è¦‹çš„ç„¡ç”¨å­—è©
        salary = re.sub(r'(æœˆè–ª|å¹´è–ª|æ™‚è–ª|å¾…é‡|è–ªè³‡|NT\$|\$)', '', salary_text)
        salary = self.clean_text(salary)

        # å¦‚æœåŒ…å«æ•¸å­—ï¼Œä¿ç•™ï¼›å¦å‰‡è¿”å›é¢è­°
        if re.search(r'\d', salary):
            return salary
        return "é¢è­°"

    def crawl_104_jobs(self, keyword, limit=10):
        """çˆ¬å– 104 äººåŠ›éŠ€è¡Œè·ç¼º"""
        jobs = []

        try:
            # 104 æœå°‹ URL
            search_url = f"https://www.104.com.tw/jobs/search/?ro=0&keyword={keyword}&expansionType=area%2Cspec%2Ccom%2Cjob%2Cwf%2Cwktm&order=15&asc=0&page=1&mode=s&jobsource=2018indexpoc"

            print(f"æ­£åœ¨æœå°‹ 104 è·ç¼ºï¼š{keyword}")
            response = self.session.get(search_url, timeout=10)

            if response.status_code != 200:
                print(f"104 è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
                return jobs

            soup = BeautifulSoup(response.text, 'html.parser')

            # æ‰¾åˆ°è·ç¼ºåˆ—è¡¨
            job_items = soup.find_all('article', class_='js-job-item')

            for item in job_items[:limit]:
                try:
                    # æå–è·ç¼ºè³‡è¨Š
                    title_elem = item.find('a', {'data-job-name': True})
                    title = self.clean_text(title_elem.get('data-job-name', '')) if title_elem else ''

                    company_elem = item.find('a', {'data-cust-name': True})
                    company = self.clean_text(company_elem.get('data-cust-name', '')) if company_elem else ''

                    location_elem = item.find('ul', class_='job-list-intro')
                    location = ''
                    if location_elem:
                        location_li = location_elem.find('li')
                        location = self.clean_text(location_li.get_text()) if location_li else ''

                    salary_elem = item.find('span', class_='job-list-tag')
                    salary = self.extract_salary(salary_elem.get_text() if salary_elem else '')

                    # è·ç¼ºé€£çµ
                    job_url = ''
                    if title_elem and title_elem.get('href'):
                        job_url = 'https:' + title_elem['href'] if title_elem['href'].startswith('//') else title_elem[
                            'href']

                    # å…¬å¸ Logoï¼ˆ104 é€šå¸¸æœ‰é è¨­åœ–ç‰‡ï¼‰
                    logo_url = "https://www.104.com.tw/img/logo_104.png"  # é è¨­ 104 Logo

                    if title and company:  # ç¢ºä¿åŸºæœ¬è³‡è¨Šå®Œæ•´
                        job_data = {
                            "id": f"104_{len(jobs) + 1}",
                            "title": title,
                            "company": company,
                            "salary": salary,
                            "location": location,
                            "url": job_url,
                            "platform": "104äººåŠ›éŠ€è¡Œ",
                            "logo_url": logo_url,
                            "description": f"{title} - {company}",
                            "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…"],
                            "tags": ["104", keyword],
                            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        jobs.append(job_data)
                        print(f"âœ… æ‰¾åˆ°è·ç¼ºï¼š{title} - {company}")

                    self.delay_random()  # éš¨æ©Ÿå»¶é²

                except Exception as e:
                    print(f"è§£æè·ç¼ºæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            print(f"ğŸ‰ 104 æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            print(f"âŒ çˆ¬å– 104 è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def crawl_1111_jobs(self, keyword, limit=10):
        """çˆ¬å– 1111 äººåŠ›éŠ€è¡Œè·ç¼º"""
        jobs = []

        try:
            # 1111 æœå°‹ URL
            search_url = f"https://www.1111.com.tw/search/job?ks={keyword}"

            print(f"æ­£åœ¨æœå°‹ 1111 è·ç¼ºï¼š{keyword}")
            response = self.session.get(search_url, timeout=10)

            if response.status_code != 200:
                print(f"1111 è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
                return jobs

            soup = BeautifulSoup(response.text, 'html.parser')

            # æ‰¾åˆ°è·ç¼ºåˆ—è¡¨ï¼ˆ1111 çš„çµæ§‹ï¼‰
            job_items = soup.find_all('div', class_='joblist_cont')

            for item in job_items[:limit]:
                try:
                    # æå–è·ç¼ºè³‡è¨Š
                    title_elem = item.find('a', class_='joblist_job_name')
                    title = self.clean_text(title_elem.get_text()) if title_elem else ''

                    company_elem = item.find('a', class_='joblist_comp_name')
                    company = self.clean_text(company_elem.get_text()) if company_elem else ''

                    location_elem = item.find('div', class_='joblist_job_condition')
                    location = self.clean_text(location_elem.get_text()) if location_elem else ''

                    salary_elem = item.find('div', class_='joblist_money')
                    salary = self.extract_salary(salary_elem.get_text() if salary_elem else '')

                    # è·ç¼ºé€£çµ
                    job_url = ''
                    if title_elem and title_elem.get('href'):
                        job_url = 'https://www.1111.com.tw' + title_elem['href']

                    logo_url = "https://www.1111.com.tw/img/logo.png"  # é è¨­ 1111 Logo

                    if title and company:
                        job_data = {
                            "id": f"1111_{len(jobs) + 1}",
                            "title": title,
                            "company": company,
                            "salary": salary,
                            "location": location,
                            "url": job_url,
                            "platform": "1111äººåŠ›éŠ€è¡Œ",
                            "logo_url": logo_url,
                            "description": f"{title} - {company}",
                            "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…"],
                            "tags": ["1111", keyword],
                            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        jobs.append(job_data)
                        print(f"âœ… æ‰¾åˆ°è·ç¼ºï¼š{title} - {company}")

                    self.delay_random()

                except Exception as e:
                    print(f"è§£æ 1111 è·ç¼ºæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            print(f"ğŸ‰ 1111 æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            print(f"âŒ çˆ¬å– 1111 è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def crawl_cakeresume_jobs(self, keyword, limit=10):
        """çˆ¬å– CakeResume è·ç¼ºï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        jobs = []

        try:
            print(f"æ­£åœ¨æœå°‹ CakeResume è·ç¼ºï¼š{keyword}")

            # CakeResume API æˆ–ç¶²é çˆ¬å–ï¼ˆéœ€è¦æ›´è¤‡é›œçš„è™•ç†ï¼‰
            # é€™è£¡å…ˆæä¾›ä¸€å€‹ç¤ºä¾‹çµæ§‹

            # ç¤ºä¾‹è·ç¼ºè³‡æ–™
            sample_jobs = [
                {
                    "id": f"cake_{1}",
                    "title": f"{keyword}å·¥ç¨‹å¸«",
                    "company": "æ–°å‰µç§‘æŠ€å…¬å¸",
                    "salary": "60,000 - 90,000",
                    "location": "å°åŒ—å¸‚",
                    "url": "https://www.cakeresume.com/jobs",
                    "platform": "CakeResume",
                    "logo_url": "https://www.cakeresume.com/favicon.ico",
                    "description": f"{keyword}ç›¸é—œè·ç¼º",
                    "requirements": ["ç›¸é—œç¶“é©—", "åœ˜éšŠåˆä½œ"],
                    "tags": ["CakeResume", keyword],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            ]

            jobs.extend(sample_jobs)
            print(f"ğŸ‰ CakeResume æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            print(f"âŒ çˆ¬å– CakeResume è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def search_all_platforms(self, keyword, limit_per_platform=5):
        """æœå°‹æ‰€æœ‰å¹³å°çš„è·ç¼º"""
        print(f"ğŸš€ é–‹å§‹æœå°‹é—œéµå­—ï¼š{keyword}")

        all_jobs = []

        # æœå°‹å„å¹³å°
        jobs_104 = self.crawl_104_jobs(keyword, limit_per_platform)
        all_jobs.extend(jobs_104)

        self.delay_random()

        jobs_1111 = self.crawl_1111_jobs(keyword, limit_per_platform)
        all_jobs.extend(jobs_1111)

        self.delay_random()

        jobs_cake = self.crawl_cakeresume_jobs(keyword, limit_per_platform)
        all_jobs.extend(jobs_cake)

        print(f"ğŸ‰ æœå°‹å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(all_jobs)} å€‹è·ç¼º")

        # å„²å­˜åˆ° JSON æª”æ¡ˆ
        self.save_jobs_to_file(all_jobs)

        return all_jobs

    def save_jobs_to_file(self, jobs):
        """å„²å­˜è·ç¼ºåˆ° JSON æª”æ¡ˆ"""
        try:
            data = {
                "jobs": jobs,
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_count": len(jobs)
            }

            with open('jobs.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ“ è·ç¼ºè³‡æ–™å·²å„²å­˜åˆ° jobs.json")

        except Exception as e:
            print(f"âŒ å„²å­˜è·ç¼ºè³‡æ–™å¤±æ•—ï¼š{e}")


# æ¸¬è©¦å‡½æ•¸
def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    crawler = JobCrawler()

    # æ¸¬è©¦æœå°‹
    jobs = crawler.search_all_platforms("Python", 3)

    print("\nğŸ“‹ æœå°‹çµæœé è¦½ï¼š")
    for job in jobs[:3]:
        print(f"â€¢ {job['title']} - {job['company']} ({job['platform']})")


if __name__ == "__main__":
    test_crawler()