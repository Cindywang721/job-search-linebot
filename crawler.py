import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime
import re
import urllib.parse


class JobCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

        # å…¬å¸Logoæ˜ å°„ - å¸¸è¦‹å…¬å¸çš„Logo
        self.company_logos = {
            'microsoft': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/44/Microsoft_logo.svg/200px-Microsoft_logo.svg.png',
            'google': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Google_2015_logo.svg/200px-Google_2015_logo.svg.png',
            'apple': 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Apple_logo_black.svg/200px-Apple_logo_black.svg.png',
            'meta': 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Meta_Platforms_Inc._logo.svg/200px-Meta_Platforms_Inc._logo.svg.png',
            'amazon': 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Amazon_logo.svg/200px-Amazon_logo.svg.png',
            'netflix': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Netflix_2015_logo.svg/200px-Netflix_2015_logo.svg.png',
            'tesla': 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Tesla_T_symbol.svg/200px-Tesla_T_symbol.svg.png',
            'spotify': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Spotify_logo_without_text.svg/200px-Spotify_logo_without_text.svg.png',
            'uber': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Uber_logo_2018.svg/200px-Uber_logo_2018.svg.png',
            'airbnb': 'https://upload.wikimedia.org/wikipedia.org/wikipedia/commons/6/69/Airbnb_Logo_BÃ©lo.svg',
            'å°ç©é›»': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TSMC.svg/200px-TSMC.svg.png',
            'tsmc': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TSMC.svg/200px-TSMC.svg.png',
            'è¯ç™¼ç§‘': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/MediaTek_logo.svg/200px-MediaTek_logo.svg.png',
            'mediatek': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/MediaTek_logo.svg/200px-MediaTek_logo.svg.png',
            'é´»æµ·': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Foxconn_logo.svg/200px-Foxconn_logo.svg.png',
            'foxconn': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Foxconn_logo.svg/200px-Foxconn_logo.svg.png'
        }

    def delay_random(self):
        """éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«åçˆ¬èŸ²åµæ¸¬"""
        time.sleep(random.uniform(2, 5))

    def clean_text(self, text):
        """æ¸…ç†æ–‡å­—ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())

    def extract_salary(self, salary_text):
        """æå–è–ªè³‡è³‡è¨Š"""
        if not salary_text:
            return "é¢è­°"

        # ç§»é™¤å¸¸è¦‹çš„ç„¡ç”¨å­—è©
        salary = re.sub(r'(æœˆè–ª|å¹´è–ª|æ™‚è–ª|å¾…é‡|è–ªè³‡|NT\$|\$)', '', salary_text)
        salary = self.clean_text(salary)

        # å¦‚æœåŒ…å«æ•¸å­—ï¼Œä¿ç•™ï¼›å¦å‰‡è¿”å›é¢è­°
        if re.search(r'\d', salary):
            return salary
        return "é¢è­°"

    def get_company_logo(self, company_name):
        """æ ¹æ“šå…¬å¸åç¨±ç²å–Logo"""
        if not company_name:
            return "https://via.placeholder.com/80x80/4285F4/FFFFFF?text=LOGO"

        company_lower = company_name.lower()

        # æª¢æŸ¥æ˜¯å¦æœ‰é è¨­çš„å…¬å¸Logo
        for key, logo_url in self.company_logos.items():
            if key in company_lower or key in company_name:
                return logo_url

        # æ ¹æ“šå…¬å¸åç¨±ç”Ÿæˆå½©è‰²Logo
        company_initial = company_name[0].upper() if company_name else 'C'
        colors = ['4285F4', 'EA4335', 'FBBC04', '34A853', 'FF6D01', '9C27B0']
        color = colors[hash(company_name) % len(colors)]

        logo_url = f"https://via.placeholder.com/80x80/{color}/FFFFFF?text={company_initial}"
        return logo_url

    def crawl_104_jobs(self, keyword, limit=10):
        """çˆ¬å– 104 äººåŠ›éŠ€è¡Œè·ç¼º - çœŸå¯¦æœå°‹"""
        jobs = []

        try:
            # 104 æœå°‹ URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.104.com.tw/jobs/search/?ro=0&keyword={encoded_keyword}&expansionType=area%2Cspec%2Ccom%2Cjob%2Cwf%2Cwktm&order=15&asc=0&page=1&mode=s&jobsource=2018indexpoc"

            print(f"ğŸ” æ­£åœ¨æœå°‹ 104 è·ç¼ºï¼š{keyword}")
            response = self.session.get(search_url, timeout=15)

            if response.status_code != 200:
                print(f"âŒ 104 è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
                return jobs

            soup = BeautifulSoup(response.text, 'html.parser')

            # 104 çš„è·ç¼ºå¡ç‰‡é¸æ“‡å™¨
            job_cards = soup.find_all('article', {'data-job-name': True}) or soup.find_all('div', class_='js-job-item')

            print(f"ğŸ” æ‰¾åˆ° {len(job_cards)} å€‹è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    # æå–è·ä½åç¨±
                    title = ""
                    title_elem = card.find('a', {'data-job-name': True})
                    if title_elem:
                        title = self.clean_text(title_elem.get('data-job-name', ''))
                    else:
                        # å‚™ç”¨æ–¹æ¡ˆ
                        title_link = card.find('a', class_='js-job-link') or card.find('h2', class_='b-tit')
                        if title_link:
                            title = self.clean_text(title_link.get_text())

                    # æå–å…¬å¸åç¨±
                    company = ""
                    company_elem = card.find('a', {'data-cust-name': True})
                    if company_elem:
                        company = self.clean_text(company_elem.get('data-cust-name', ''))
                    else:
                        # å‚™ç”¨æ–¹æ¡ˆ
                        company_link = card.find('a', class_='js-company-link') or card.find('ul',
                                                                                             class_='b-list-inline')
                        if company_link:
                            company = self.clean_text(company_link.get_text())

                    # æå–åœ°é»
                    location = ""
                    location_elem = card.find('ul', class_='job-list-intro') or card.find('ul', class_='b-list-inline')
                    if location_elem:
                        location_items = location_elem.find_all('li')
                        for item in location_items:
                            text = self.clean_text(item.get_text())
                            if any(loc in text for loc in
                                   ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'æ–°ç«¹', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'é ç«¯']):
                                location = text
                                break
                        if not location and location_items:
                            location = self.clean_text(location_items[0].get_text())

                    # æå–è–ªè³‡
                    salary = "é¢è­°"
                    salary_elem = card.find('span', class_='b-tag') or card.find('span', class_='job-list-tag')
                    if salary_elem:
                        salary_text = salary_elem.get_text()
                        if 'è¬' in salary_text or 'åƒ' in salary_text or ' in salary_text or any(char.isdigit() for char in salary_text):
                            salary = self.extract_salary(salary_text)

                    # æå–è·ç¼ºé€£çµ
                    job_url = ""
                    if title_elem and title_elem.get('href'):
                        href = title_elem['href']
                        if href.startswith('//'):
                            job_url = 'https:' + href
                        elif href.startswith('/'):
                            job_url = 'https://www.104.com.tw' + href
                        else:
                            job_url = href

                    # æå–å·¥ä½œæè¿°
                    description = f"{title} - {company}"
                    desc_elem = card.find('p', class_='job-list-intro') or card.find('div', class_='job-desc')
                    if desc_elem:
                        desc_text = self.clean_text(desc_elem.get_text())
                        if desc_text:
                            description = desc_text[:100] + "..." if len(desc_text) > 100 else desc_text

                    # ç²å–å…¬å¸Logo
                    logo_url = self.get_company_logo(company)

                    if title and company:  # ç¢ºä¿åŸºæœ¬è³‡è¨Šå®Œæ•´
                        job_data = {
                            "id": f"104_{int(time.time())}_{i}",
                            "title": title,
                            "company": company,
                            "salary": salary,
                            "location": location or "æœªæä¾›",
                            "url": job_url or f"https://www.104.com.tw/jobs/search/?keyword={encoded_keyword}",
                            "platform": "104äººåŠ›éŠ€è¡Œ",
                            "logo_url": logo_url,
                            "description": description,
                            "requirements": ["è«‹æŸ¥çœ‹å€‹åˆ¥è·ç¼ºè©³æƒ…", "å…·ç›¸é—œå·¥ä½œç¶“é©—å„ªå…ˆ"],
                            "tags": ["104", keyword.lower()],
                            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        jobs.append(job_data)
                        print(f"âœ… 104è·ç¼º {i + 1}: {title} - {company}")

                    self.delay_random()  # éš¨æ©Ÿå»¶é²

                except Exception as e:
                    print(f"âŒ è§£æ104è·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            print(f"ğŸ‰ 104 æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            print(f"âŒ çˆ¬å– 104 è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def crawl_1111_jobs(self, keyword, limit=10):
        """çˆ¬å– 1111 äººåŠ›éŠ€è¡Œè·ç¼º - çœŸå¯¦æœå°‹"""
        jobs = []

        try:
            # 1111 æœå°‹ URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.1111.com.tw/search/job?ks={encoded_keyword}"

            print(f"ğŸ” æ­£åœ¨æœå°‹ 1111 è·ç¼ºï¼š{keyword}")
            response = self.session.get(search_url, timeout=15)

            if response.status_code != 200:
                print(f"âŒ 1111 è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
                return jobs

            soup = BeautifulSoup(response.text, 'html.parser')

            # 1111 çš„è·ç¼ºåˆ—è¡¨
            job_items = soup.find_all('div', class_='joblist_cont') or soup.find_all('article', class_='job-item')

            print(f"ğŸ” æ‰¾åˆ° {len(job_items)} å€‹è·ç¼ºå…ƒç´ ")

            for i, item in enumerate(job_items[:limit]):
                try:
                    # æå–è·ä½åç¨±
                    title = ""
                    title_elem = item.find('a', class_='joblist_job_name') or item.find('h4', class_='job-title')
                    if title_elem:
                        title = self.clean_text(title_elem.get_text())

                    # æå–å…¬å¸åç¨±
                    company = ""
                    company_elem = item.find('a', class_='joblist_comp_name') or item.find('h5', class_='company-name')
                    if company_elem:
                        company = self.clean_text(company_elem.get_text())

                    # æå–åœ°é»
                    location = ""
                    location_elem = item.find('div', class_='joblist_job_condition') or item.find('span',
                                                                                                  class_='job-location')
                    if location_elem:
                        location_text = self.clean_text(location_elem.get_text())
                        # æå–åœ°é»é—œéµå­—
                        for loc in ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'æ–°ç«¹', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'é ç«¯']:
                            if loc in location_text:
                                location = loc
                                break
                        if not location:
                            location = location_text[:10] if location_text else "æœªæä¾›"

                    # æå–è–ªè³‡
                    salary = "é¢è­°"
                    salary_elem = item.find('div', class_='joblist_money') or item.find('span', class_='job-salary')
                    if salary_elem:
                        salary = self.extract_salary(salary_elem.get_text())

                    # æå–è·ç¼ºé€£çµ
                    job_url = ""
                    if title_elem and title_elem.get('href'):
                        href = title_elem['href']
                        if href.startswith('/'):
                            job_url = 'https://www.1111.com.tw' + href
                        else:
                            job_url = href

                    # ç²å–å…¬å¸Logo
                    logo_url = self.get_company_logo(company)

                    # å·¥ä½œæè¿°
                    description = f"{company} çš„ {title} è·ä½"

                    if title and company:
                        job_data = {
                            "id": f"1111_{int(time.time())}_{i}",
                            "title": title,
                            "company": company,
                            "salary": salary,
                            "location": location or "æœªæä¾›",
                            "url": job_url or f"https://www.1111.com.tw/search/job?ks={encoded_keyword}",
                            "platform": "1111äººåŠ›éŠ€è¡Œ",
                            "logo_url": logo_url,
                            "description": description,
                            "requirements": ["è«‹æŸ¥çœ‹å€‹åˆ¥è·ç¼ºè©³æƒ…", "å…·ç›¸é—œå·¥ä½œç¶“é©—å„ªå…ˆ"],
                            "tags": ["1111", keyword.lower()],
                            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        jobs.append(job_data)
                        print(f"âœ… 1111è·ç¼º {i + 1}: {title} - {company}")

                    self.delay_random()

                except Exception as e:
                    print(f"âŒ è§£æ1111è·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            print(f"ğŸ‰ 1111 æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            print(f"âŒ çˆ¬å– 1111 è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def crawl_yourator_jobs(self, keyword, limit=10):
        """çˆ¬å– Yourator è·ç¼º - çœŸå¯¦æœå°‹"""
        jobs = []

        try:
            # Yourator æœå°‹ URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.yourator.co/jobs?q={encoded_keyword}"

            print(f"ğŸ” æ­£åœ¨æœå°‹ Yourator è·ç¼ºï¼š{keyword}")
            response = self.session.get(search_url, timeout=15)

            if response.status_code != 200:
                print(f"âŒ Yourator è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
                return jobs

            soup = BeautifulSoup(response.text, 'html.parser')

            # æ ¹æ“šå¯¦éš›çš„HTMLçµæ§‹èª¿æ•´é¸æ“‡å™¨
            job_cards = soup.find_all('div', class_='job-item') or soup.find_all('a', class_='job-link')

            for i, card in enumerate(job_cards[:limit]):
                try:
                    # åŸºæ–¼Youratorçš„å¯¦éš›çµæ§‹æå–è³‡è¨Š
                    title_elem = card.find('h3') or card.find('h4') or card.find('div', class_='job-title')
                    title = self.clean_text(title_elem.get_text()) if title_elem else ""

                    company_elem = card.find('div', class_='company-name') or card.find('span', class_='company')
                    company = self.clean_text(company_elem.get_text()) if company_elem else ""

                    # ç²å–å…¬å¸Logo
                    logo_url = self.get_company_logo(company)

                    if title and company:
                        job_data = {
                            "id": f"yourator_{int(time.time())}_{i}",
                            "title": title,
                            "company": company,
                            "salary": "é¢è­°",
                            "location": "å°åŒ—å¸‚",
                            "url": f"https://www.yourator.co/jobs?q={encoded_keyword}",
                            "platform": "Yourator",
                            "logo_url": logo_url,
                            "description": f"{title} - {company}",
                            "requirements": ["è«‹æŸ¥çœ‹å€‹åˆ¥è·ç¼ºè©³æƒ…"],
                            "tags": ["yourator", keyword.lower()],
                            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        jobs.append(job_data)
                        print(f"âœ… Youratorè·ç¼º {i + 1}: {title} - {company}")

                    self.delay_random()

                except Exception as e:
                    print(f"âŒ è§£æYouratorè·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            print(f"ğŸ‰ Yourator æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            print(f"âŒ çˆ¬å– Yourator è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def search_all_platforms(self, keyword, limit_per_platform=5):
        """æœå°‹æ‰€æœ‰å¹³å°çš„è·ç¼º"""
        print(f"ğŸš€ é–‹å§‹æœå°‹é—œéµå­—ï¼š{keyword}")

        all_jobs = []

        # æœå°‹ 104 äººåŠ›éŠ€è¡Œ
        try:
            jobs_104 = self.crawl_104_jobs(keyword, limit_per_platform)
            all_jobs.extend(jobs_104)
        except Exception as e:
            print(f"âŒ 104æœå°‹å¤±æ•—ï¼š{e}")

        self.delay_random()

        # æœå°‹ 1111 äººåŠ›éŠ€è¡Œ
        try:
            jobs_1111 = self.crawl_1111_jobs(keyword, limit_per_platform)
            all_jobs.extend(jobs_1111)
        except Exception as e:
            print(f"âŒ 1111æœå°‹å¤±æ•—ï¼š{e}")

        self.delay_random()

        # æœå°‹ Yourator
        try:
            jobs_yourator = self.crawl_yourator_jobs(keyword, limit_per_platform)
            all_jobs.extend(jobs_yourator)
        except Exception as e:
            print(f"âŒ Youratoræœå°‹å¤±æ•—ï¼š{e}")

        print(f"ğŸ‰ æœå°‹å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(all_jobs)} å€‹è·ç¼º")

        # å„²å­˜åˆ° JSON æª”æ¡ˆ
        self.save_jobs_to_file(all_jobs, keyword)

        return all_jobs

    def save_jobs_to_file(self, jobs, keyword=""):
        """å„²å­˜è·ç¼ºåˆ° JSON æª”æ¡ˆ"""
        try:
            data = {
                "jobs": jobs,
                "keyword": keyword,
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_count": len(jobs)
            }

            with open('jobs.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ“ è·ç¼ºè³‡æ–™å·²å„²å­˜åˆ° jobs.json")

        except Exception as e:
            print(f"âŒ å„²å­˜è·ç¼ºè³‡æ–™å¤±æ•—ï¼š{e}")


# æ¸¬è©¦å‡½æ•¸
def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    crawler = JobCrawler()

    # æ¸¬è©¦æœå°‹
    test_keywords = ["Python", "å‰ç«¯å·¥ç¨‹å¸«", "æ•¸æ“šåˆ†æ"]

    for keyword in test_keywords:
        print(f"\nğŸ§ª æ¸¬è©¦é—œéµå­—: {keyword}")
        jobs = crawler.search_all_platforms(keyword, 2)

        print(f"\nğŸ“‹ æœå°‹çµæœé è¦½ï¼š")
        for job in jobs[:3]:
            print(f"â€¢ {job['title']} - {job['company']} ({job['platform']})")
            print(f"  ğŸ’° {job['salary']} | ğŸ“ {job['location']}")
            print(f"  ğŸ¢ Logo: {job['logo_url']}")
            print()


if __name__ == "__main__":
    test_crawler()