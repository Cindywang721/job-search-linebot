import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime
import re
import urllib.parse
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedJobCrawler:
    def __init__(self):
        # ç°¡åŒ–çš„ User-Agent åˆ—è¡¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
        ]

        self.session = requests.Session()

        # åŸºç¤ headers
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

    def get_headers(self):
        """ç²å–éš¨æ©Ÿ User-Agent"""
        headers = self.headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        return headers

    def delay_random(self, min_delay=1, max_delay=3):
        """éš¨æ©Ÿå»¶é²"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def safe_request(self, url, max_retries=2):
        """å®‰å…¨çš„ HTTP è«‹æ±‚"""
        for attempt in range(max_retries):
            try:
                headers = self.get_headers()
                response = self.session.get(url, headers=headers, timeout=15)

                if response.status_code == 200:
                    logger.info(f"âœ… æˆåŠŸè«‹æ±‚: {url[:50]}...")
                    return response
                else:
                    logger.warning(f"âš ï¸ è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")

            except Exception as e:
                logger.error(f"âŒ è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                self.delay_random(3, 6)

        return None

    def crawl_104_jobs_real(self, keyword, location="", limit=10):
        """çœŸå¯¦çˆ¬å– 104 äººåŠ›éŠ€è¡Œè·ç¼º"""
        jobs = []

        try:
            # æ§‹å»º 104 æœå°‹ URL
            encoded_keyword = urllib.parse.quote(keyword)

            # 104 çœŸå¯¦æœå°‹ URL æ ¼å¼
            search_url = f"https://www.104.com.tw/jobs/search/?ro=0&kwop=7&keyword={encoded_keyword}&expansionType=area%2Cspec%2Ccom%2Cjob%2Cwf%2Cwktm&order=15&asc=0&page=1&mode=s&jobsource=2018indexpoc"

            if location:
                # åœ°å€ä»£ç¢¼æ˜ å°„
                area_codes = {
                    'å°åŒ—': '6001001000',
                    'æ–°åŒ—': '6001002000',
                    'æ¡ƒåœ’': '6001005000',
                    'æ–°ç«¹': '6001004000',
                    'å°ä¸­': '6001008000',
                    'å°å—': '6001014000',
                    'é«˜é›„': '6001016000'
                }

                area_code = area_codes.get(location, '')
                if area_code:
                    search_url += f"&area={area_code}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ 104ï¼š{keyword} (åœ°é»ï¼š{location or 'ä¸é™'})")
            logger.info(f"ğŸŒ æœå°‹ç¶²å€ï¼š{search_url}")

            response = self.safe_request(search_url)

            if not response:
                logger.error("âŒ ç„¡æ³•é€£æ¥åˆ° 104 ç¶²ç«™")
                return []

            soup = BeautifulSoup(response.text, 'html.parser')

            # 104 è·ç¼ºå¡ç‰‡çš„å¤šç¨®å¯èƒ½é¸æ“‡å™¨
            job_selectors = [
                'article[data-job-name]',  # ä¸»è¦é¸æ“‡å™¨
                'div.js-job-item',  # å‚™ç”¨é¸æ“‡å™¨1
                'div[data-job-name]',  # å‚™ç”¨é¸æ“‡å™¨2
                'article.js-job-item'  # å‚™ç”¨é¸æ“‡å™¨3
            ]

            job_cards = []
            for selector in job_selectors:
                job_cards = soup.select(selector)
                if job_cards:
                    logger.info(f"âœ… ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(job_cards)} å€‹è·ç¼º")
                    break
                else:
                    logger.warning(f"âš ï¸ é¸æ“‡å™¨ '{selector}' æ²’æœ‰æ‰¾åˆ°è·ç¼º")

            if not job_cards:
                logger.error("âŒ ä½¿ç”¨æ‰€æœ‰é¸æ“‡å™¨éƒ½æ²’æœ‰æ‰¾åˆ°è·ç¼º")
                # å˜—è©¦æŸ¥çœ‹é é¢å…§å®¹
                logger.info(f"ğŸ“„ é é¢å…§å®¹é è¦½ï¼š{response.text[:500]}...")
                return []

            processed_jobs = 0
            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_104_job_card_real(card, i)
                    if job_data:
                        jobs.append(job_data)
                        processed_jobs += 1
                        logger.info(f"âœ… 104è·ç¼º {processed_jobs}: {job_data['title']} - {job_data['company']}")
                        logger.info(f"ğŸ”— è·ç¼ºé€£çµ: {job_data['url']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"âŒ è§£æ104è·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            logger.info(f"ğŸ‰ 104 æœå°‹å®Œæˆï¼ŒæˆåŠŸè§£æ {len(jobs)} å€‹è·ç¼º")

        except Exception as e:
            logger.error(f"âŒ çˆ¬å– 104 è·ç¼ºå¤±æ•—ï¼š{e}")

        return jobs

    def _parse_104_job_card_real(self, card, index):
        """çœŸå¯¦è§£æ 104 è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±å’Œé€£çµ - å¤šç¨®æ–¹å¼å˜—è©¦
            title = ""
            job_url = ""

            # æ–¹æ³•1ï¼šå¾ data-job-name å±¬æ€§ç²å–
            if card.get('data-job-name'):
                title = card.get('data-job-name', '').strip()

            # æ–¹æ³•2ï¼šå¾é€£çµå…ƒç´ ç²å–
            title_link = card.find('a', {'data-job-name': True})
            if title_link:
                if not title:
                    title = title_link.get('data-job-name', '').strip()
                job_url = title_link.get('href', '')

            # æ–¹æ³•3ï¼šå¾æ¨™é¡Œå…ƒç´ ç²å–
            if not title:
                title_elem = card.find('h2') or card.find('a', class_='js-job-link')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title_elem.name == 'a':
                        job_url = title_elem.get('href', '')

            # å…¬å¸åç¨± - å¤šç¨®æ–¹å¼å˜—è©¦
            company = ""

            # æ–¹æ³•1ï¼šå¾ data-cust-name å±¬æ€§
            company_elem = card.find('a', {'data-cust-name': True})
            if company_elem:
                company = company_elem.get('data-cust-name', '').strip()

            # æ–¹æ³•2ï¼šå¾å…¬å¸é€£çµ
            if not company:
                company_link = card.find('a', class_='js-company-link')
                if company_link:
                    company = company_link.get_text(strip=True)

            # æ–¹æ³•3ï¼šå¾åˆ—è¡¨ä¸­æŸ¥æ‰¾
            if not company:
                ul_elem = card.find('ul', class_='b-list-inline')
                if ul_elem:
                    company_li = ul_elem.find('li')
                    if company_li:
                        company = company_li.get_text(strip=True)

            # è–ªè³‡è³‡è¨Š
            salary = self._extract_salary_from_104_card(card)

            # åœ°é»è³‡è¨Š
            location = self._extract_location_from_104_card(card)

            # è™•ç†è·ç¼ºé€£çµ
            if job_url:
                if job_url.startswith('//'):
                    job_url = 'https:' + job_url
                elif job_url.startswith('/'):
                    job_url = 'https://www.104.com.tw' + job_url
                elif not job_url.startswith('http'):
                    job_url = 'https://www.104.com.tw' + job_url
            else:
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°é€£çµï¼Œå˜—è©¦æ§‹å»ºæœå°‹é€£çµ
                encoded_title = urllib.parse.quote(title) if title else urllib.parse.quote("è·ç¼º")
                job_url = f"https://www.104.com.tw/jobs/search/?keyword={encoded_title}"

            # é©—è­‰å¿…è¦æ¬„ä½
            if not title or not company:
                logger.warning(f"âš ï¸ è·ç¼ºè³‡è¨Šä¸å®Œæ•´ - æ¨™é¡Œ: '{title}', å…¬å¸: '{company}'")
                return None

            job_data = {
                "id": f"104_{int(time.time())}_{index}",
                "title": self.clean_text(title),
                "company": self.clean_text(company),
                "salary": salary,
                "location": location,
                "url": job_url,
                "platform": "104äººåŠ›éŠ€è¡Œ",
                "logo_url": self.get_company_logo(company),
                "description": f"{title} - {company}",
                "requirements": ["è«‹é»æ“Šè·ç¼ºé€£çµæŸ¥çœ‹è©³ç´°è¦æ±‚", "å…·ç›¸é—œå·¥ä½œç¶“é©—å„ªå…ˆ"],
                "tags": ["104", "æ­£è·"],
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            return job_data

        except Exception as e:
            logger.error(f"âŒ è§£æ 104 å¡ç‰‡å¤±æ•—ï¼š{e}")
            return None

    def _extract_salary_from_104_card(self, card):
        """å¾ 104 å¡ç‰‡ä¸­æå–è–ªè³‡"""
        salary_selectors = [
            'span.b-tag',  # ä¸»è¦è–ªè³‡æ¨™ç±¤
            'div.job-list-tag',  # å·¥ä½œæ¨™ç±¤å€åŸŸ
            'span.b-tag--default',  # é è¨­æ¨™ç±¤
            '.b-tag'  # é€šç”¨æ¨™ç±¤
        ]

        for selector in salary_selectors:
            salary_elem = card.select_one(selector)
            if salary_elem:
                text = salary_elem.get_text(strip=True)
                # æª¢æŸ¥æ˜¯å¦åŒ…å«è–ªè³‡é—œéµå­—
                if any(keyword in text for keyword in ['è¬', 'åƒ', '

    def get_headers(self):
        """ç²å–éš¨æ©Ÿ User-Agent"""
        headers = self.headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        return headers

    def delay_random(self, min_delay=1, max_delay=3):
        """éš¨æ©Ÿå»¶é²"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def safe_request(self, url, max_retries=2):
        """å®‰å…¨çš„ HTTP è«‹æ±‚"""
        for attempt in range(max_retries):
            try:
                headers = self.get_headers()
                response = self.session.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    return response
                elif response.status_code == 429:
                    logger.warning("è¢«é™åˆ¶è¨ªå•ï¼Œç­‰å¾…å¾Œé‡è©¦")
                    time.sleep(10)
                else:
                    logger.warning(f"è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")

            except Exception as e:
                logger.error(f"è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                self.delay_random(3, 6)

        return None

    def create_sample_jobs(self, keyword, location="", limit=5):
        """å»ºç«‹ç¯„ä¾‹è·ç¼ºï¼ˆç•¶çˆ¬èŸ²å¤±æ•—æ™‚ä½¿ç”¨ï¼‰"""
        sample_jobs = []

        job_templates = [
            {
                "title": f"{keyword}å·¥ç¨‹å¸«",
                "company": "ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸",
                "salary": "æœˆè–ª 50,000 - 80,000",
                "location": location or "å°åŒ—å¸‚",
                "platform": "104äººåŠ›éŠ€è¡Œ",
                "description": f"è² è²¬ {keyword} ç›¸é—œç³»çµ±é–‹ç™¼èˆ‡ç¶­è­·",
                "requirements": ["ç†Ÿæ‚‰ç›¸é—œæŠ€è¡“", "å…·åœ˜éšŠåˆä½œç²¾ç¥", "2å¹´ä»¥ä¸Šå·¥ä½œç¶“é©—"]
            },
            {
                "title": f"è³‡æ·±{keyword}é–‹ç™¼è€…",
                "company": "å‰µæ–°ç§‘æŠ€å…¬å¸",
                "salary": "æœˆè–ª 70,000 - 120,000",
                "location": location or "æ–°ç«¹å¸‚",
                "platform": "CakeResume",
                "description": f"åƒèˆ‡ {keyword} ç”¢å“è¨­è¨ˆèˆ‡é–‹ç™¼",
                "requirements": ["3å¹´ä»¥ä¸Šç›¸é—œç¶“é©—", "ç†Ÿæ‚‰æ•æ·é–‹ç™¼", "è‰¯å¥½æºé€šèƒ½åŠ›"]
            },
            {
                "title": f"{keyword}æŠ€è¡“å°ˆå®¶",
                "company": "æ–°å‰µåœ˜éšŠ",
                "salary": "é¢è­°",
                "location": location or "å°ä¸­å¸‚",
                "platform": "Yourator",
                "description": f"é ˜å° {keyword} æŠ€è¡“åœ˜éšŠï¼Œåˆ¶å®šæŠ€è¡“ç­–ç•¥",
                "requirements": ["5å¹´ä»¥ä¸Šç¶“é©—", "é ˜å°ç¶“é©—", "æŠ€è¡“å‰ç»æ€§"]
            }
        ]

        for i, template in enumerate(job_templates[:limit]):
            job_data = {
                "id": f"sample_{int(time.time())}_{i}",
                "title": template["title"],
                "company": template["company"],
                "salary": template["salary"],
                "location": template["location"],
                "url": f"https://example.com/job/{i}",
                "platform": template["platform"],
                "logo_url": self.get_company_logo(template["company"]),
                "description": template["description"],
                "requirements": template["requirements"],
                "tags": [template["platform"].lower(), keyword.lower()],
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            sample_jobs.append(job_data)

        return sample_jobs

    def crawl_104_jobs(self, keyword, location="", salary_min="", salary_max="", limit=10):
        """çˆ¬å– 104 äººåŠ›éŠ€è¡Œè·ç¼º"""
        jobs = []

        try:
            # æ§‹å»ºæœå°‹ URL
            params = {
                'ro': '0',
                'keyword': keyword,
                'order': '15',
                'asc': '0',
                'page': '1',
                'mode': 's'
            }

            if location:
                params['area'] = location
            if salary_min:
                params['sal1'] = salary_min
            if salary_max:
                params['sal2'] = salary_max

            search_url = f"https://www.104.com.tw/jobs/search/?{urllib.parse.urlencode(params)}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ 104 è·ç¼ºï¼š{keyword}")
            response = self.safe_request(search_url)

            if not response:
                logger.warning("104 æœå°‹å¤±æ•—ï¼Œä½¿ç”¨ç¯„ä¾‹è³‡æ–™")
                return self.create_sample_jobs(keyword, location, min(limit, 2))

            soup = BeautifulSoup(response.text, 'html.parser')

            # å°‹æ‰¾è·ç¼ºå…ƒç´ 
            job_cards = soup.find_all('article', class_='js-job-item') or \
                        soup.find_all('div', class_='job-list-item') or \
                        soup.find_all('article', {'data-job-name': True})

            logger.info(f"æ‰¾åˆ° {len(job_cards)} å€‹ 104 è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_104_job_card(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"âœ… 104è·ç¼º {i + 1}: {job_data['title']} - {job_data['company']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"è§£æ104è·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

        except Exception as e:
            logger.error(f"çˆ¬å– 104 è·ç¼ºå¤±æ•—ï¼š{e}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è·ç¼ºï¼Œè¿”å›ç¯„ä¾‹è³‡æ–™
        if not jobs:
            jobs = self.create_sample_jobs(keyword, location, min(limit, 2))

        return jobs

    def _parse_104_job_card(self, card, index):
        """è§£æ 104 è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±
            title_elem = card.find('a', {'data-job-name': True}) or \
                         card.find('h2', class_='js-job-link') or \
                         card.find('a', class_='js-job-link')

            title = ""
            job_url = ""

            if title_elem:
                title = title_elem.get('data-job-name') or title_elem.get_text(strip=True)
                job_url = title_elem.get('href', '')

            # å…¬å¸åç¨±
            company_elem = card.find('a', {'data-cust-name': True}) or \
                           card.find('ul', class_='b-list-inline')

            company = ""
            if company_elem:
                company = company_elem.get('data-cust-name') or company_elem.get_text(strip=True)

            # åœ°é»å’Œè–ªè³‡
            location = self._extract_location_from_card(card)
            salary = self._extract_salary_from_card(card)

            # å®Œæ•´ URL
            if job_url and not job_url.startswith('http'):
                if job_url.startswith('//'):
                    job_url = 'https:' + job_url
                elif job_url.startswith('/'):
                    job_url = 'https://www.104.com.tw' + job_url

            if title and company:
                return {
                    "id": f"104_{int(time.time())}_{index}",
                    "title": self.clean_text(title),
                    "company": self.clean_text(company),
                    "salary": salary,
                    "location": location,
                    "url": job_url or "https://www.104.com.tw",
                    "platform": "104äººåŠ›éŠ€è¡Œ",
                    "logo_url": self.get_company_logo(company),
                    "description": f"{title} - {company}",
                    "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…", "å…·ç›¸é—œå·¥ä½œç¶“é©—"],
                    "tags": ["104", "æ­£è·"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

        except Exception as e:
            logger.error(f"è§£æ 104 å¡ç‰‡å¤±æ•—ï¼š{e}")

        return None

    def crawl_cakeresume_jobs(self, keyword, location="", limit=10):
        """çˆ¬å– CakeResume è·ç¼º"""
        jobs = []

        try:
            params = {
                'q': keyword,
                'location': location,
                'page': 1
            }

            search_url = f"https://www.cakeresume.com/jobs?{urllib.parse.urlencode(params)}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ CakeResume è·ç¼ºï¼š{keyword}")
            response = self.safe_request(search_url)

            if not response:
                logger.warning("CakeResume æœå°‹å¤±æ•—ï¼Œä½¿ç”¨ç¯„ä¾‹è³‡æ–™")
                return self.create_sample_jobs(keyword, location, min(limit, 2))

            soup = BeautifulSoup(response.text, 'html.parser')

            # å°‹æ‰¾è·ç¼ºå…ƒç´ 
            job_cards = soup.find_all('div', class_='JobSearchItem') or \
                        soup.find_all('div', class_='job-item') or \
                        soup.find_all('a', class_='job-item-link')

            logger.info(f"æ‰¾åˆ° {len(job_cards)} å€‹ CakeResume è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_cakeresume_job_card(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"âœ… CakeResumeè·ç¼º {i + 1}: {job_data['title']} - {job_data['company']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"è§£æCakeResumeè·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

        except Exception as e:
            logger.error(f"çˆ¬å– CakeResume è·ç¼ºå¤±æ•—ï¼š{e}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è·ç¼ºï¼Œè¿”å›ç¯„ä¾‹è³‡æ–™
        if not jobs:
            jobs = self.create_sample_jobs(keyword, location, min(limit, 2))

        return jobs

    def _parse_cakeresume_job_card(self, card, index):
        """è§£æ CakeResume è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±
            title_elem = card.find('h3') or card.find('a', class_='job-title')
            title = title_elem.get_text(strip=True) if title_elem else ""

            # å…¬å¸åç¨±
            company_elem = card.find('div', class_='company-name') or \
                           card.find('span', class_='company')
            company = company_elem.get_text(strip=True) if company_elem else ""

            # é€£çµ
            link_elem = card.find('a') if card.name != 'a' else card
            job_url = link_elem.get('href', '') if link_elem else ""

            # åœ°é»å’Œè–ªè³‡
            location_elem = card.find('div', class_='location')
            location = location_elem.get_text(strip=True) if location_elem else "æœªæä¾›"

            salary_elem = card.find('div', class_='salary')
            salary = salary_elem.get_text(strip=True) if salary_elem else "é¢è­°"

            # å®Œæ•´ URL
            if job_url and not job_url.startswith('http'):
                job_url = 'https://www.cakeresume.com' + job_url

            if title and company:
                return {
                    "id": f"cakeresume_{int(time.time())}_{index}",
                    "title": self.clean_text(title),
                    "company": self.clean_text(company),
                    "salary": salary,
                    "location": location,
                    "url": job_url or "https://www.cakeresume.com",
                    "platform": "CakeResume",
                    "logo_url": self.get_company_logo(company),
                    "description": f"{title} - {company}",
                    "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…"],
                    "tags": ["cakeresume", "æ­£è·"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

        except Exception as e:
            logger.error(f"è§£æ CakeResume å¡ç‰‡å¤±æ•—ï¼š{e}")

        return None

    def crawl_yourator_jobs(self, keyword, location="", limit=10):
        """çˆ¬å– Yourator è·ç¼º"""
        jobs = []

        try:
            params = {
                'q': keyword,
                'location[]': location if location else '',
                'page': 1
            }

            search_url = f"https://www.yourator.co/jobs?{urllib.parse.urlencode(params)}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ Yourator è·ç¼ºï¼š{keyword}")
            response = self.safe_request(search_url)

            if not response:
                logger.warning("Yourator æœå°‹å¤±æ•—ï¼Œä½¿ç”¨ç¯„ä¾‹è³‡æ–™")
                return self.create_sample_jobs(keyword, location, min(limit, 2))

            soup = BeautifulSoup(response.text, 'html.parser')

            # å°‹æ‰¾è·ç¼ºå…ƒç´ 
            job_cards = soup.find_all('div', class_='job-card') or \
                        soup.find_all('a', class_='job-link')

            logger.info(f"æ‰¾åˆ° {len(job_cards)} å€‹ Yourator è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_yourator_job_card(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"âœ… Youratorè·ç¼º {i + 1}: {job_data['title']} - {job_data['company']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"è§£æYouratorè·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

        except Exception as e:
            logger.error(f"çˆ¬å– Yourator è·ç¼ºå¤±æ•—ï¼š{e}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è·ç¼ºï¼Œè¿”å›ç¯„ä¾‹è³‡æ–™
        if not jobs:
            jobs = self.create_sample_jobs(keyword, location, min(limit, 2))

        return jobs

    def _parse_yourator_job_card(self, card, index):
        """è§£æ Yourator è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±
            title_elem = card.find('h3') or card.find('div', class_='job-title')
            title = title_elem.get_text(strip=True) if title_elem else ""

            # å…¬å¸åç¨±
            company_elem = card.find('div', class_='company-name') or \
                           card.find('span', class_='company')
            company = company_elem.get_text(strip=True) if company_elem else ""

            # é€£çµ
            link_elem = card.find('a') if card.name != 'a' else card
            job_url = link_elem.get('href', '') if link_elem else ""

            # åœ°é»å’Œè–ªè³‡
            location_elem = card.find('div', class_='location')
            location = location_elem.get_text(strip=True) if location_elem else "æœªæä¾›"

            salary_elem = card.find('div', class_='salary')
            salary = salary_elem.get_text(strip=True) if salary_elem else "é¢è­°"

            # å®Œæ•´ URL
            if job_url and not job_url.startswith('http'):
                job_url = 'https://www.yourator.co' + job_url

            if title and company:
                return {
                    "id": f"yourator_{int(time.time())}_{index}",
                    "title": self.clean_text(title),
                    "company": self.clean_text(company),
                    "salary": salary,
                    "location": location,
                    "url": job_url or "https://www.yourator.co",
                    "platform": "Yourator",
                    "logo_url": self.get_company_logo(company),
                    "description": f"{title} - {company}",
                    "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…"],
                    "tags": ["yourator", "æ–°å‰µ"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

        except Exception as e:
            logger.error(f"è§£æ Yourator å¡ç‰‡å¤±æ•—ï¼š{e}")

        return None

    def search_all_platforms(self, keyword, location="", salary_min="", salary_max="", limit_per_platform=5):
        """æœå°‹æ‰€æœ‰å¹³å°çš„è·ç¼º"""
        logger.info(f"ğŸš€ é–‹å§‹å…¨å¹³å°æœå°‹ï¼š{keyword}")

        all_jobs = []

        # æœå°‹ 104
        try:
            jobs_104 = self.crawl_104_jobs(keyword, location, salary_min, salary_max, limit_per_platform)
            all_jobs.extend(jobs_104)
            logger.info(f"104 æ‰¾åˆ° {len(jobs_104)} å€‹è·ç¼º")
        except Exception as e:
            logger.error(f"104æœå°‹å¤±æ•—ï¼š{e}")

        self.delay_random(2, 4)

        # æœå°‹ CakeResume
        try:
            jobs_cake = self.crawl_cakeresume_jobs(keyword, location, limit_per_platform)
            all_jobs.extend(jobs_cake)
            logger.info(f"CakeResume æ‰¾åˆ° {len(jobs_cake)} å€‹è·ç¼º")
        except Exception as e:
            logger.error(f"CakeResumeæœå°‹å¤±æ•—ï¼š{e}")

        self.delay_random(2, 4)

        # æœå°‹ Yourator
        try:
            jobs_yourator = self.crawl_yourator_jobs(keyword, location, limit_per_platform)
            all_jobs.extend(jobs_yourator)
            logger.info(f"Yourator æ‰¾åˆ° {len(jobs_yourator)} å€‹è·ç¼º")
        except Exception as e:
            logger.error(f"Youratoræœå°‹å¤±æ•—ï¼š{e}")

        logger.info(f"ğŸ‰ å…¨å¹³å°æœå°‹å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(all_jobs)} å€‹è·ç¼º")

        # å»é‡
        all_jobs = self.deduplicate_jobs(all_jobs)

        return all_jobs

    def deduplicate_jobs(self, jobs):
        """å»é™¤é‡è¤‡è·ç¼º"""
        seen = set()
        unique_jobs = []

        for job in jobs:
            key = f"{job['company']}_{job['title']}".lower()
            if key not in seen:
                seen.add(key)
                unique_jobs.append(job)

        logger.info(f"å»é‡å¾Œå‰©é¤˜ {len(unique_jobs)} å€‹è·ç¼º")
        return unique_jobs

    def _extract_location_from_card(self, card):
        """å¾å¡ç‰‡ä¸­æå–åœ°é»è³‡è¨Š"""
        location_selectors = [
            'ul.b-list-inline li',
            'div.job-list-intro',
            'span.location',
            'div.location'
        ]

        for selector in location_selectors:
            elements = card.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if any(loc in text for loc in ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'æ–°ç«¹', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'é ç«¯']):
                    return text[:20]

        return "æœªæä¾›"

    def _extract_salary_from_card(self, card):
        """å¾å¡ç‰‡ä¸­æå–è–ªè³‡è³‡è¨Š"""
        salary_selectors = [
            'span.b-tag',
            'div.salary',
            'span.salary',
            'div.job-list-tag'
        ]

        for selector in salary_selectors:
            elem = card.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if any(keyword in text for keyword in ['è¬', 'åƒ', ', 'k', 'K']) or any(c.isdigit() for c in text):
                    return self.extract_salary(text)

        return "é¢è­°"

    def clean_text(self, text):
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())

    def extract_salary(self, salary_text):
        """æå–è–ªè³‡è³‡è¨Š"""
        if not salary_text:
            return "é¢è­°"

        salary = re.sub(r'(æœˆè–ª|å¹´è–ª|æ™‚è–ª|å¾…é‡|è–ªè³‡|NT\$|\$)', '', salary_text)
        salary = self.clean_text(salary)

        if re.search(r'\d', salary):
            return salary
        return "é¢è­°"

    def get_company_logo(self, company_name):
        """ç²å–å…¬å¸Logo"""
        if not company_name:
            return "https://via.placeholder.com/80x80/4285F4/FFFFFF?text=C"

        company_initial = company_name[0].upper() if company_name else 'C'
        colors = ['4285F4', 'EA4335', 'FBBC04', '34A853', 'FF6D01', '9C27B0']
        color = colors[hash(company_name) % len(colors)]

        return f"https://via.placeholder.com/80x80/{color}/FFFFFF?text={company_initial}"


def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    crawler = EnhancedJobCrawler()

    print("æ¸¬è©¦æœå°‹åŠŸèƒ½...")
    jobs = crawler.search_all_platforms("Python", "å°åŒ—", limit_per_platform=2)

    print(f"\næ‰¾åˆ° {len(jobs)} å€‹è·ç¼º:")
    for job in jobs:
        print(f"â€¢ {job['title']} - {job['company']} ({job['platform']})")
        print(f"  è–ªè³‡: {job['salary']} | åœ°é»: {job['location']}")


if __name__ == "__main__":
    test_crawler(), 'k', 'K', 'è–ª']) and any(c.isdigit() for c in text):
    return self.clean_text(text)

    return "é¢è­°"


def _extract_location_from_104_card(self, card):
    """å¾ 104 å¡ç‰‡ä¸­æå–åœ°é»"""
    location_selectors = [
        'ul.b-list-inline li',
        'div.job-list-intro',
        'span.job-list-intro',
        '.b-list-inline li'
    ]

    for selector in location_selectors:
        elements = card.select(selector)
        for elem in elements:
            text = elem.get_text(strip=True)
            # æª¢æŸ¥æ˜¯å¦åŒ…å«åœ°é»é—œéµå­—
            if any(loc in text for loc in ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'æ–°ç«¹', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'é ç«¯']):
                return text[:20]  # é™åˆ¶é•·åº¦

    return "æœªæä¾›"


def search_all_platforms(self, keyword, location="", salary_min="", salary_max="", limit_per_platform=5):
    """æœå°‹æ‰€æœ‰å¹³å°çš„è·ç¼º"""
    logger.info(f"ğŸš€ é–‹å§‹å…¨å¹³å°æœå°‹ï¼š{keyword}")
    logger.info(f"ğŸ“ åœ°é»ï¼š{location or 'ä¸é™'}")
    logger.info(f"ğŸ’° è–ªè³‡ï¼š{salary_min or 'ä¸é™'} - {salary_max or 'ä¸é™'}")

    all_jobs = []

    # å„ªå…ˆæœå°‹ 104ï¼ˆæœ€å¤§å¹³å°ï¼‰
    try:
        jobs_104 = self.crawl_104_jobs_real(keyword, location, limit_per_platform)
        all_jobs.extend(jobs_104)
        logger.info(f"âœ… 104 æ‰¾åˆ° {len(jobs_104)} å€‹è·ç¼º")
    except Exception as e:
        logger.error(f"âŒ 104 æœå°‹å¤±æ•—ï¼š{e}")

    # å¦‚æœ 104 æ²’æœ‰çµæœï¼Œæä¾›ç¯„ä¾‹è·ç¼º
    if not all_jobs:
        logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°çœŸå¯¦è·ç¼ºï¼Œæä¾›ç¯„ä¾‹è·ç¼º")
        sample_jobs = self.create_sample_jobs_with_real_links(keyword, location, limit_per_platform)
        all_jobs.extend(sample_jobs)

    logger.info(f"ğŸ‰ æœå°‹å®Œæˆï¼ç¸½å…± {len(all_jobs)} å€‹è·ç¼º")

    return all_jobs


def create_sample_jobs_with_real_links(self, keyword, location="", limit=5):
    """å»ºç«‹æœ‰çœŸå¯¦é€£çµçš„ç¯„ä¾‹è·ç¼º"""
    sample_jobs = []

    # çœŸå¯¦çš„æœå°‹é€£çµ
    encoded_keyword = urllib.parse.quote(keyword)
    search_urls = [
        f"https://www.104.com.tw/jobs/search/?keyword={encoded_keyword}",
        f"https://www.cakeresume.com/jobs?q={encoded_keyword}",
        f"https://www.yourator.co/jobs?q={encoded_keyword}"
    ]

    job_templates = [
        {
            "title": f"{keyword}å·¥ç¨‹å¸«",
            "company": "ç§‘æŠ€å‰µæ–°è‚¡ä»½æœ‰é™å…¬å¸",
            "salary": "æœˆè–ª 45,000 - 75,000",
            "platform": "104äººåŠ›éŠ€è¡Œ",
            "url": search_urls[0]
        },
        {
            "title": f"è³‡æ·±{keyword}é–‹ç™¼è€…",
            "company": "æ•¸ä½ç§‘æŠ€æœ‰é™å…¬å¸",
            "salary": "æœˆè–ª 60,000 - 100,000",
            "platform": "CakeResume",
            "url": search_urls[1]
        },
        {
            "title": f"{keyword}æŠ€è¡“å°ˆå®¶",
            "company": "æ–°å‰µç§‘æŠ€åœ˜éšŠ",
            "salary": "æœˆè–ª 70,000 - 120,000",
            "platform": "Yourator",
            "url": search_urls[2]
        }
    ]

    for i, template in enumerate(job_templates[:limit]):
        job_data = {
            "id": f"sample_{int(time.time())}_{i}",
            "title": template["title"],
            "company": template["company"],
            "salary": template["salary"],
            "location": location or "å°åŒ—å¸‚",
            "url": template["url"],  # çœŸå¯¦æœå°‹é€£çµ
            "platform": template["platform"],
            "logo_url": self.get_company_logo(template["company"]),
            "description": f"è² è²¬ {keyword} ç›¸é—œç³»çµ±é–‹ç™¼èˆ‡ç¶­è­·ï¼Œèˆ‡åœ˜éšŠå”ä½œå®Œæˆå°ˆæ¡ˆç›®æ¨™",
            "requirements": [
                f"ç†Ÿæ‚‰ {keyword} ç›¸é—œæŠ€è¡“",
                "å…·å‚™è‰¯å¥½çš„åœ˜éšŠåˆä½œèƒ½åŠ›",
                "æœ‰ç›¸é—œå·¥ä½œç¶“é©—è€…å„ªå…ˆ"
            ],
            "tags": [template["platform"].lower(), keyword.lower()],
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        sample_jobs.append(job_data)

    return sample_jobs


def clean_text(self, text):
    """æ¸…ç†æ–‡å­—"""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text.strip())


def get_company_logo(self, company_name):
    """ç²å–å…¬å¸Logo"""
    if not company_name:
        return "https://via.placeholder.com/80x80/4285F4/FFFFFF?text=C"

    company_initial = company_name[0].upper() if company_name else 'C'
    colors = ['4285F4', 'EA4335', 'FBBC04', '34A853', 'FF6D01', '9C27B0']
    color = colors[hash(company_name) % len(colors)]

    return f"https://via.placeholder.com/80x80/{color}/FFFFFF?text={company_initial}"


def test_real_crawler():
    """æ¸¬è©¦çœŸå¯¦çˆ¬èŸ²åŠŸèƒ½"""
    crawler = EnhancedJobCrawler()

    test_keywords = ["è»Ÿé«”å·¥ç¨‹å¸«", "ç”¢å“ç¶“ç†", "æ•¸æ“šåˆ†æå¸«", "å‰ç«¯å·¥ç¨‹å¸«"]

    for keyword in test_keywords:
        print(f"\nğŸ” æ¸¬è©¦æœå°‹ï¼š{keyword}")
        jobs = crawler.search_all_platforms(keyword, "å°åŒ—", limit_per_platform=2)

        for job in jobs:
            print(f"â€¢ {job['title']} - {job['company']}")
            print(f"  ğŸ’° {job['salary']} | ğŸ“ {job['location']}")
            print(f"  ğŸ”— {job['url']}")
            print()


if __name__ == "__main__":
    test_real_crawler()


    def get_headers(self):
        """ç²å–éš¨æ©Ÿ User-Agent"""
        headers = self.headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        return headers


    def delay_random(self, min_delay=1, max_delay=3):
        """éš¨æ©Ÿå»¶é²"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)


    def safe_request(self, url, max_retries=2):
        """å®‰å…¨çš„ HTTP è«‹æ±‚"""
        for attempt in range(max_retries):
            try:
                headers = self.get_headers()
                response = self.session.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    return response
                elif response.status_code == 429:
                    logger.warning("è¢«é™åˆ¶è¨ªå•ï¼Œç­‰å¾…å¾Œé‡è©¦")
                    time.sleep(10)
                else:
                    logger.warning(f"è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")

            except Exception as e:
                logger.error(f"è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                self.delay_random(3, 6)

        return None


    def create_sample_jobs(self, keyword, location="", limit=5):
        """å»ºç«‹ç¯„ä¾‹è·ç¼ºï¼ˆç•¶çˆ¬èŸ²å¤±æ•—æ™‚ä½¿ç”¨ï¼‰"""
        sample_jobs = []

        job_templates = [
            {
                "title": f"{keyword}å·¥ç¨‹å¸«",
                "company": "ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸",
                "salary": "æœˆè–ª 50,000 - 80,000",
                "location": location or "å°åŒ—å¸‚",
                "platform": "104äººåŠ›éŠ€è¡Œ",
                "description": f"è² è²¬ {keyword} ç›¸é—œç³»çµ±é–‹ç™¼èˆ‡ç¶­è­·",
                "requirements": ["ç†Ÿæ‚‰ç›¸é—œæŠ€è¡“", "å…·åœ˜éšŠåˆä½œç²¾ç¥", "2å¹´ä»¥ä¸Šå·¥ä½œç¶“é©—"]
            },
            {
                "title": f"è³‡æ·±{keyword}é–‹ç™¼è€…",
                "company": "å‰µæ–°ç§‘æŠ€å…¬å¸",
                "salary": "æœˆè–ª 70,000 - 120,000",
                "location": location or "æ–°ç«¹å¸‚",
                "platform": "CakeResume",
                "description": f"åƒèˆ‡ {keyword} ç”¢å“è¨­è¨ˆèˆ‡é–‹ç™¼",
                "requirements": ["3å¹´ä»¥ä¸Šç›¸é—œç¶“é©—", "ç†Ÿæ‚‰æ•æ·é–‹ç™¼", "è‰¯å¥½æºé€šèƒ½åŠ›"]
            },
            {
                "title": f"{keyword}æŠ€è¡“å°ˆå®¶",
                "company": "æ–°å‰µåœ˜éšŠ",
                "salary": "é¢è­°",
                "location": location or "å°ä¸­å¸‚",
                "platform": "Yourator",
                "description": f"é ˜å° {keyword} æŠ€è¡“åœ˜éšŠï¼Œåˆ¶å®šæŠ€è¡“ç­–ç•¥",
                "requirements": ["5å¹´ä»¥ä¸Šç¶“é©—", "é ˜å°ç¶“é©—", "æŠ€è¡“å‰ç»æ€§"]
            }
        ]

        for i, template in enumerate(job_templates[:limit]):
            job_data = {
                "id": f"sample_{int(time.time())}_{i}",
                "title": template["title"],
                "company": template["company"],
                "salary": template["salary"],
                "location": template["location"],
                "url": f"https://example.com/job/{i}",
                "platform": template["platform"],
                "logo_url": self.get_company_logo(template["company"]),
                "description": template["description"],
                "requirements": template["requirements"],
                "tags": [template["platform"].lower(), keyword.lower()],
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            sample_jobs.append(job_data)

        return sample_jobs


    def crawl_104_jobs(self, keyword, location="", salary_min="", salary_max="", limit=10):
        """çˆ¬å– 104 äººåŠ›éŠ€è¡Œè·ç¼º"""
        jobs = []

        try:
            # æ§‹å»ºæœå°‹ URL
            params = {
                'ro': '0',
                'keyword': keyword,
                'order': '15',
                'asc': '0',
                'page': '1',
                'mode': 's'
            }

            if location:
                params['area'] = location
            if salary_min:
                params['sal1'] = salary_min
            if salary_max:
                params['sal2'] = salary_max

            search_url = f"https://www.104.com.tw/jobs/search/?{urllib.parse.urlencode(params)}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ 104 è·ç¼ºï¼š{keyword}")
            response = self.safe_request(search_url)

            if not response:
                logger.warning("104 æœå°‹å¤±æ•—ï¼Œä½¿ç”¨ç¯„ä¾‹è³‡æ–™")
                return self.create_sample_jobs(keyword, location, min(limit, 2))

            soup = BeautifulSoup(response.text, 'html.parser')

            # å°‹æ‰¾è·ç¼ºå…ƒç´ 
            job_cards = soup.find_all('article', class_='js-job-item') or \
                        soup.find_all('div', class_='job-list-item') or \
                        soup.find_all('article', {'data-job-name': True})

            logger.info(f"æ‰¾åˆ° {len(job_cards)} å€‹ 104 è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_104_job_card(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"âœ… 104è·ç¼º {i + 1}: {job_data['title']} - {job_data['company']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"è§£æ104è·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

        except Exception as e:
            logger.error(f"çˆ¬å– 104 è·ç¼ºå¤±æ•—ï¼š{e}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è·ç¼ºï¼Œè¿”å›ç¯„ä¾‹è³‡æ–™
        if not jobs:
            jobs = self.create_sample_jobs(keyword, location, min(limit, 2))

        return jobs


    def _parse_104_job_card(self, card, index):
        """è§£æ 104 è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±
            title_elem = card.find('a', {'data-job-name': True}) or \
                         card.find('h2', class_='js-job-link') or \
                         card.find('a', class_='js-job-link')

            title = ""
            job_url = ""

            if title_elem:
                title = title_elem.get('data-job-name') or title_elem.get_text(strip=True)
                job_url = title_elem.get('href', '')

            # å…¬å¸åç¨±
            company_elem = card.find('a', {'data-cust-name': True}) or \
                           card.find('ul', class_='b-list-inline')

            company = ""
            if company_elem:
                company = company_elem.get('data-cust-name') or company_elem.get_text(strip=True)

            # åœ°é»å’Œè–ªè³‡
            location = self._extract_location_from_card(card)
            salary = self._extract_salary_from_card(card)

            # å®Œæ•´ URL
            if job_url and not job_url.startswith('http'):
                if job_url.startswith('//'):
                    job_url = 'https:' + job_url
                elif job_url.startswith('/'):
                    job_url = 'https://www.104.com.tw' + job_url

            if title and company:
                return {
                    "id": f"104_{int(time.time())}_{index}",
                    "title": self.clean_text(title),
                    "company": self.clean_text(company),
                    "salary": salary,
                    "location": location,
                    "url": job_url or "https://www.104.com.tw",
                    "platform": "104äººåŠ›éŠ€è¡Œ",
                    "logo_url": self.get_company_logo(company),
                    "description": f"{title} - {company}",
                    "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…", "å…·ç›¸é—œå·¥ä½œç¶“é©—"],
                    "tags": ["104", "æ­£è·"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

        except Exception as e:
            logger.error(f"è§£æ 104 å¡ç‰‡å¤±æ•—ï¼š{e}")

        return None


    def crawl_cakeresume_jobs(self, keyword, location="", limit=10):
        """çˆ¬å– CakeResume è·ç¼º"""
        jobs = []

        try:
            params = {
                'q': keyword,
                'location': location,
                'page': 1
            }

            search_url = f"https://www.cakeresume.com/jobs?{urllib.parse.urlencode(params)}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ CakeResume è·ç¼ºï¼š{keyword}")
            response = self.safe_request(search_url)

            if not response:
                logger.warning("CakeResume æœå°‹å¤±æ•—ï¼Œä½¿ç”¨ç¯„ä¾‹è³‡æ–™")
                return self.create_sample_jobs(keyword, location, min(limit, 2))

            soup = BeautifulSoup(response.text, 'html.parser')

            # å°‹æ‰¾è·ç¼ºå…ƒç´ 
            job_cards = soup.find_all('div', class_='JobSearchItem') or \
                        soup.find_all('div', class_='job-item') or \
                        soup.find_all('a', class_='job-item-link')

            logger.info(f"æ‰¾åˆ° {len(job_cards)} å€‹ CakeResume è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_cakeresume_job_card(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"âœ… CakeResumeè·ç¼º {i + 1}: {job_data['title']} - {job_data['company']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"è§£æCakeResumeè·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

        except Exception as e:
            logger.error(f"çˆ¬å– CakeResume è·ç¼ºå¤±æ•—ï¼š{e}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è·ç¼ºï¼Œè¿”å›ç¯„ä¾‹è³‡æ–™
        if not jobs:
            jobs = self.create_sample_jobs(keyword, location, min(limit, 2))

        return jobs


    def _parse_cakeresume_job_card(self, card, index):
        """è§£æ CakeResume è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±
            title_elem = card.find('h3') or card.find('a', class_='job-title')
            title = title_elem.get_text(strip=True) if title_elem else ""

            # å…¬å¸åç¨±
            company_elem = card.find('div', class_='company-name') or \
                           card.find('span', class_='company')
            company = company_elem.get_text(strip=True) if company_elem else ""

            # é€£çµ
            link_elem = card.find('a') if card.name != 'a' else card
            job_url = link_elem.get('href', '') if link_elem else ""

            # åœ°é»å’Œè–ªè³‡
            location_elem = card.find('div', class_='location')
            location = location_elem.get_text(strip=True) if location_elem else "æœªæä¾›"

            salary_elem = card.find('div', class_='salary')
            salary = salary_elem.get_text(strip=True) if salary_elem else "é¢è­°"

            # å®Œæ•´ URL
            if job_url and not job_url.startswith('http'):
                job_url = 'https://www.cakeresume.com' + job_url

            if title and company:
                return {
                    "id": f"cakeresume_{int(time.time())}_{index}",
                    "title": self.clean_text(title),
                    "company": self.clean_text(company),
                    "salary": salary,
                    "location": location,
                    "url": job_url or "https://www.cakeresume.com",
                    "platform": "CakeResume",
                    "logo_url": self.get_company_logo(company),
                    "description": f"{title} - {company}",
                    "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…"],
                    "tags": ["cakeresume", "æ­£è·"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

        except Exception as e:
            logger.error(f"è§£æ CakeResume å¡ç‰‡å¤±æ•—ï¼š{e}")

        return None


    def crawl_yourator_jobs(self, keyword, location="", limit=10):
        """çˆ¬å– Yourator è·ç¼º"""
        jobs = []

        try:
            params = {
                'q': keyword,
                'location[]': location if location else '',
                'page': 1
            }

            search_url = f"https://www.yourator.co/jobs?{urllib.parse.urlencode(params)}"

            logger.info(f"ğŸ” æ­£åœ¨æœå°‹ Yourator è·ç¼ºï¼š{keyword}")
            response = self.safe_request(search_url)

            if not response:
                logger.warning("Yourator æœå°‹å¤±æ•—ï¼Œä½¿ç”¨ç¯„ä¾‹è³‡æ–™")
                return self.create_sample_jobs(keyword, location, min(limit, 2))

            soup = BeautifulSoup(response.text, 'html.parser')

            # å°‹æ‰¾è·ç¼ºå…ƒç´ 
            job_cards = soup.find_all('div', class_='job-card') or \
                        soup.find_all('a', class_='job-link')

            logger.info(f"æ‰¾åˆ° {len(job_cards)} å€‹ Yourator è·ç¼ºå…ƒç´ ")

            for i, card in enumerate(job_cards[:limit]):
                try:
                    job_data = self._parse_yourator_job_card(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"âœ… Youratorè·ç¼º {i + 1}: {job_data['title']} - {job_data['company']}")

                    self.delay_random(0.5, 1.5)

                except Exception as e:
                    logger.error(f"è§£æYouratorè·ç¼º {i + 1} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

        except Exception as e:
            logger.error(f"çˆ¬å– Yourator è·ç¼ºå¤±æ•—ï¼š{e}")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è·ç¼ºï¼Œè¿”å›ç¯„ä¾‹è³‡æ–™
        if not jobs:
            jobs = self.create_sample_jobs(keyword, location, min(limit, 2))

        return jobs


    def _parse_yourator_job_card(self, card, index):
        """è§£æ Yourator è·ç¼ºå¡ç‰‡"""
        try:
            # è·ä½åç¨±
            title_elem = card.find('h3') or card.find('div', class_='job-title')
            title = title_elem.get_text(strip=True) if title_elem else ""

            # å…¬å¸åç¨±
            company_elem = card.find('div', class_='company-name') or \
                           card.find('span', class_='company')
            company = company_elem.get_text(strip=True) if company_elem else ""

            # é€£çµ
            link_elem = card.find('a') if card.name != 'a' else card
            job_url = link_elem.get('href', '') if link_elem else ""

            # åœ°é»å’Œè–ªè³‡
            location_elem = card.find('div', class_='location')
            location = location_elem.get_text(strip=True) if location_elem else "æœªæä¾›"

            salary_elem = card.find('div', class_='salary')
            salary = salary_elem.get_text(strip=True) if salary_elem else "é¢è­°"

            # å®Œæ•´ URL
            if job_url and not job_url.startswith('http'):
                job_url = 'https://www.yourator.co' + job_url

            if title and company:
                return {
                    "id": f"yourator_{int(time.time())}_{index}",
                    "title": self.clean_text(title),
                    "company": self.clean_text(company),
                    "salary": salary,
                    "location": location,
                    "url": job_url or "https://www.yourator.co",
                    "platform": "Yourator",
                    "logo_url": self.get_company_logo(company),
                    "description": f"{title} - {company}",
                    "requirements": ["è«‹æŸ¥çœ‹è·ç¼ºè©³æƒ…"],
                    "tags": ["yourator", "æ–°å‰µ"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

        except Exception as e:
            logger.error(f"è§£æ Yourator å¡ç‰‡å¤±æ•—ï¼š{e}")

        return None


    def search_all_platforms(self, keyword, location="", salary_min="", salary_max="", limit_per_platform=5):
        """æœå°‹æ‰€æœ‰å¹³å°çš„è·ç¼º"""
        logger.info(f"ğŸš€ é–‹å§‹å…¨å¹³å°æœå°‹ï¼š{keyword}")

        all_jobs = []

        # æœå°‹ 104
        try:
            jobs_104 = self.crawl_104_jobs(keyword, location, salary_min, salary_max, limit_per_platform)
            all_jobs.extend(jobs_104)
            logger.info(f"104 æ‰¾åˆ° {len(jobs_104)} å€‹è·ç¼º")
        except Exception as e:
            logger.error(f"104æœå°‹å¤±æ•—ï¼š{e}")

        self.delay_random(2, 4)

        # æœå°‹ CakeResume
        try:
            jobs_cake = self.crawl_cakeresume_jobs(keyword, location, limit_per_platform)
            all_jobs.extend(jobs_cake)
            logger.info(f"CakeResume æ‰¾åˆ° {len(jobs_cake)} å€‹è·ç¼º")
        except Exception as e:
            logger.error(f"CakeResumeæœå°‹å¤±æ•—ï¼š{e}")

        self.delay_random(2, 4)

        # æœå°‹ Yourator
        try:
            jobs_yourator = self.crawl_yourator_jobs(keyword, location, limit_per_platform)
            all_jobs.extend(jobs_yourator)
            logger.info(f"Yourator æ‰¾åˆ° {len(jobs_yourator)} å€‹è·ç¼º")
        except Exception as e:
            logger.error(f"Youratoræœå°‹å¤±æ•—ï¼š{e}")

        logger.info(f"ğŸ‰ å…¨å¹³å°æœå°‹å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(all_jobs)} å€‹è·ç¼º")

        # å»é‡
        all_jobs = self.deduplicate_jobs(all_jobs)

        return all_jobs


    def deduplicate_jobs(self, jobs):
        """å»é™¤é‡è¤‡è·ç¼º"""
        seen = set()
        unique_jobs = []

        for job in jobs:
            key = f"{job['company']}_{job['title']}".lower()
            if key not in seen:
                seen.add(key)
                unique_jobs.append(job)

        logger.info(f"å»é‡å¾Œå‰©é¤˜ {len(unique_jobs)} å€‹è·ç¼º")
        return unique_jobs


    def _extract_location_from_card(self, card):
        """å¾å¡ç‰‡ä¸­æå–åœ°é»è³‡è¨Š"""
        location_selectors = [
            'ul.b-list-inline li',
            'div.job-list-intro',
            'span.location',
            'div.location'
        ]

        for selector in location_selectors:
            elements = card.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if any(loc in text for loc in ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'æ–°ç«¹', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'é ç«¯']):
                    return text[:20]

        return "æœªæä¾›"


    def _extract_salary_from_card(self, card):
        """å¾å¡ç‰‡ä¸­æå–è–ªè³‡è³‡è¨Š"""
        salary_selectors = [
            'span.b-tag',
            'div.salary',
            'span.salary',
            'div.job-list-tag'
        ]

        for selector in salary_selectors:
            elem = card.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if any(keyword in text for keyword in ['è¬', 'åƒ', ', 'k', 'K']) or any(c.isdigit() for c in text):
                    return self.extract_salary(text)

        return "é¢è­°"


    def clean_text(self, text):
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())


    def extract_salary(self, salary_text):
        """æå–è–ªè³‡è³‡è¨Š"""
        if not salary_text:
            return "é¢è­°"

        salary = re.sub(r'(æœˆè–ª|å¹´è–ª|æ™‚è–ª|å¾…é‡|è–ªè³‡|NT\$|\$)', '', salary_text)
        salary = self.clean_text(salary)

        if re.search(r'\d', salary):
            return salary
        return "é¢è­°"


    def get_company_logo(self, company_name):
        """ç²å–å…¬å¸Logo"""
        if not company_name:
            return "https://via.placeholder.com/80x80/4285F4/FFFFFF?text=C"

        company_initial = company_name[0].upper() if company_name else 'C'
        colors = ['4285F4', 'EA4335', 'FBBC04', '34A853', 'FF6D01', '9C27B0']
        color = colors[hash(company_name) % len(colors)]

        return f"https://via.placeholder.com/80x80/{color}/FFFFFF?text={company_initial}"


def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    crawler = EnhancedJobCrawler()

    print("æ¸¬è©¦æœå°‹åŠŸèƒ½...")
    jobs = crawler.search_all_platforms("Python", "å°åŒ—", limit_per_platform=2)

    print(f"\næ‰¾åˆ° {len(jobs)} å€‹è·ç¼º:")
    for job in jobs:
        print(f"â€¢ {job['title']} - {job['company']} ({job['platform']})")
        print(f"  è–ªè³‡: {job['salary']} | åœ°é»: {job['location']}")


if __name__ == "__main__":
    test_crawler()